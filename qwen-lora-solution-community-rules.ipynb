{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":363127,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":301507,"modelId":322000},{"sourceId":391622,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":322458,"modelId":322000},{"sourceId":426330,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":347541,"modelId":368803}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile constants.py\n#BASE_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nBASE_MODEL_PATH = \"/kaggle/input/qwen-3/transformers/0.6b-base/1\"\n\nLORA_PATH = \"output/\"\nCOMPLETE = \"Answer:\"\nprompt = f\"You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Answer 'yes' or 'no' only.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:41:31.459956Z","iopub.execute_input":"2025-09-26T10:41:31.460236Z","iopub.status.idle":"2025-09-26T10:41:31.466219Z","shell.execute_reply.started":"2025-09-26T10:41:31.460207Z","shell.execute_reply":"2025-09-26T10:41:31.465289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntrain_data = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\n\nprint(len(train_data))\ntest_sample=    pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\").sample(frac=0.5,random_state = 42 )\n\nprint(len(test_sample))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:41:33.065328Z","iopub.execute_input":"2025-09-26T10:41:33.065607Z","iopub.status.idle":"2025-09-26T10:41:33.213012Z","shell.execute_reply.started":"2025-09-26T10:41:33.065585Z","shell.execute_reply":"2025-09-26T10:41:33.212323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch import Tensor\nimport kagglehub\n\n\n# Pooling function (same as before)\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n    # Check whether padding is on the left\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\n# OOP wrapper around tokenizer + model + pooling\nclass QwenEmbedder(nn.Module):\n    def __init__(self, model_dir: str, max_length: int = 8192, device: str = None):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, padding_side=\"left\")\n        self.model = AutoModel.from_pretrained(model_dir)\n        self.max_length = max_length\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to(self.device)  # move model to device\n\n    def forward(self, texts: list[str]) -> Tensor:\n        # Tokenize batch\n        batch_dict = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        ).to(self.device)\n\n        # Forward pass\n        outputs = self.model(**batch_dict)\n\n        # Pool to sequence embedding\n        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n\n        # Normalize embeddings\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings\n\nif __name__ == \"__main__\":\n    model_dir = kagglehub.model_download(\"qwen-lm/qwen-3-embedding/transformers/0.6b\")\n    embedder = QwenEmbedder(model_dir)\n\n    # Queries and docs\n    queries = [\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\",\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"\n    ]\n    documents = [\n        \"The capital of China is Beijing.\",\n        \"Gravity is a force that attracts two bodies towards each other...\"\n    ]\n\n    # Get embeddings\n    query_emb = embedder(queries)\n    doc_emb = embedder(documents)\n\n    # Compute similarity\n    scores = query_emb @ doc_emb.T\n    print(scores.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\n\nimport pandas as pd \nfrom constants import prompt, COMPLETE\nimport numpy as np \nimport random\nimport re\nfrom datasets import Dataset\n\nrandom.seed(42)\nnp.random.seed(42)\n\ndef url_to_semantics(txt : str) -> str:\n    if not isinstance(txt,str):\n        return \"\"\n    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n    urls = re.findall(url_pattern, txt)\n    \n    if not urls:\n        return \"\" \n\n    all_semantics = []\n    seen_semantics = set()\n\n    for url in urls:\n        url_lower = url.lower()\n        \n        domain_match = re.search(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\", url_lower)\n        if domain_match:\n            full_domain = domain_match.group(1)\n            parts = full_domain.split('.')\n            for part in parts:\n                if part and part not in seen_semantics and len(part) > 3: # Avoid short parts like 'www'\n                    all_semantics.append(f\"domain:{part}\")\n                    seen_semantics.add(part)\n\n        # Extract path parts\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", url_lower)\n        path_parts = [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()] # Split by common delimiters\n\n        for part in path_parts:\n            # Clean up potential file extensions or query params\n            part_clean = re.sub(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\", \"\", part)\n            if part_clean and part_clean not in seen_semantics and len(part_clean) > 3:\n                all_semantics.append(f\"path:{part_clean}\")\n                seen_semantics.add(part_clean)\n\n    if not all_semantics:\n        return \"\"\n\n    return f\"\\nURL Keywords: {' '.join(all_semantics)}\"\n\ndef build_prompt(row):\n    subreddit = row.get(\"subreddit\", \"unknown\")\n    rule = row.get(\"rule\", \"\")\n    pos_example = row.get(\"positive_example\", \"\")\n    neg_example = row.get(\"negative_example\", \"\")\n    body = row.get(\"body\", \"\")\n    url_features_body = url_to_semantics(body)\n    url_features_pos = url_to_semantics(pos_example)\n    url_features_neg = url_to_semantics(neg_example)\n    return f\"\"\"\n{prompt}\n\nr/{subreddit} \nrule: {rule}\nExamples : \n1) {pos_example}{url_features_pos}\n{COMPLETE} yes\n2) {neg_example}{url_features_neg}\n{COMPLETE} no\n\n------\nComment: {body}{url_features_body}\n{COMPLETE} \"\"\"\n\n\ndef get_data_for_training(fpath,sample_frac = 0.5):\n    train_data = pd.read_csv(f\"{fpath}/train.csv\")\n    \n    test_df= pd.read_csv(f\"{fpath}/test.csv\").sample(frac=sample_frac,random_state = 42 )\n\n    \n\n\n    train_df = train_data[['body','rule','subreddit','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2','rule_violation']]\n    \n    #randomly assign examples\n    train_df['positive_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['positive_example_1'],train_df['positive_example_2'])\n    train_df['negative_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['negative_example_1'], train_df['negative_example_2'])\n    train_df.drop(columns = ['positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'], inplace = True)\n\n    dfs = [train_df]\n    \n    # build test df \n    \n    for rule_violation in ['yes', 'no']:\n        for i in range(1,3): #loop through both examples\n            subdf =  test_df.copy().drop(columns=['body','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'])\n\n            if rule_violation == 'yes':   # case when rule is violated \n                subdf['body'] = test_df[f'positive_example_{i}']\n                subdf['positive_example'] = test_df[f'positive_example_{3-i}']\n                subdf['negative_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'negative_example_{i}'],test_df[f'negative_example_{3-i}'])\n                subdf['rule_violation'] = 1\n            else:  # case when rule is not violated \n                subdf['body'] = test_df[f'negative_example_{i}']\n                subdf['positive_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'positive_example_{i}'],test_df[f'positive_example_{3-i}'])\n\n                subdf['neagtive_example'] = test_df[f'negative_example_{3-i}']\n                subdf['rule_violation'] = 0\n            dfs.append(subdf)\n\n    df =  pd.concat(dfs, axis = 0).drop_duplicates(ignore_index = True)\n    \n    return df\n\ndef build_dataset(df,is_train = True):\n    df['prompt'] = df.apply(build_prompt, axis = 1)\n\n    if is_train:\n\n        df['completion'] = df['rule_violation'].map(\n            {\n                1 : 'yes',\n                0 : 'no'\n            }\n        )\n        df = df[['prompt','completion']]\n    else: \n        df = df[['prompt']]\n\n    print(df)\n\n    dataset = Dataset.from_pandas(df)\n    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:29:26.270251Z","iopub.execute_input":"2025-09-26T11:29:26.270598Z","iopub.status.idle":"2025-09-26T11:29:26.278716Z","shell.execute_reply.started":"2025-09-26T11:29:26.27056Z","shell.execute_reply":"2025-09-26T11:29:26.278045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:41:44.74127Z","iopub.execute_input":"2025-09-26T10:41:44.741547Z","iopub.status.idle":"2025-09-26T10:43:06.982397Z","shell.execute_reply.started":"2025-09-26T10:41:44.741525Z","shell.execute_reply":"2025-09-26T10:43:06.981628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:43:06.983764Z","iopub.execute_input":"2025-09-26T10:43:06.984035Z","iopub.status.idle":"2025-09-26T10:43:35.208378Z","shell.execute_reply.started":"2025-09-26T10:43:06.984011Z","shell.execute_reply":"2025-09-26T10:43:35.207849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:43:35.209024Z","iopub.execute_input":"2025-09-26T10:43:35.209205Z","iopub.status.idle":"2025-09-26T10:43:40.887647Z","shell.execute_reply.started":"2025-09-26T10:43:35.209189Z","shell.execute_reply":"2025-09-26T10:43:40.886939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, HTML\nfrom utils import get_data_for_training, build_dataset, build_prompt, url_to_semantics\n\n# Lora imports\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom tqdm.auto import tqdm\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom constants import LORA_PATH, BASE_MODEL_PATH\n\n\ndef main():\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    df = get_data_for_training(data_path)\n    train_dataset = build_dataset(df)\n    df_train = pd.DataFrame(train_dataset)\n\n    df_train = pd.DataFrame(train_dataset)\n\n    lora_config = LoraConfig(\n        r = 16,\n        lora_alpha= 32,\n        lora_dropout = 0.1,\n        bias = \"none\",\n        target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type = \"CAUSAL_LM\")\n    \n    training_args = SFTConfig(\n        num_train_epochs = 1,\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 4,\n        optim = \"paged_adamw_8bit\",\n        learning_rate = 5e-5,\n        weight_decay = 0.01,\n        max_grad_norm = 1.0,\n\n        lr_scheduler_type = \"cosine\",\n        warmup_ratio=0.05,\n\n        bf16= is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n\n        save_strategy= \"no\",\n        report_to = \"none\",\n\n        completion_only_loss = True,\n        packing = True,\n        remove_unused_columns = False\n    )\n    \n    trainer = SFTTrainer(\n        BASE_MODEL_PATH,\n        args = training_args,\n        train_dataset = train_dataset,\n        peft_config = lora_config\n    )\n\n    trainer.train()\n    trainer.save_model(LORA_PATH)\n    #print(df_train.head(10))\n    \n\nif __name__ == \"__main__\":\n    main()\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:43:40.889998Z","iopub.execute_input":"2025-09-26T10:43:40.890229Z","iopub.status.idle":"2025-09-26T10:43:40.89635Z","shell.execute_reply.started":"2025-09-26T10:43:40.890207Z","shell.execute_reply":"2025-09-26T10:43:40.89544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:43:40.897022Z","iopub.execute_input":"2025-09-26T10:43:40.897313Z","iopub.status.idle":"2025-09-26T10:48:09.655805Z","shell.execute_reply.started":"2025-09-26T10:43:40.897288Z","shell.execute_reply":"2025-09-26T10:48:09.654787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install logits_processor_zoo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:48:09.657129Z","iopub.execute_input":"2025-09-26T10:48:09.657472Z","iopub.status.idle":"2025-09-26T10:48:18.090333Z","shell.execute_reply.started":"2025-09-26T10:48:09.657413Z","shell.execute_reply":"2025-09-26T10:48:18.089548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install autoawq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:48:18.091421Z","iopub.execute_input":"2025-09-26T10:48:18.091676Z","iopub.status.idle":"2025-09-26T10:48:23.757501Z","shell.execute_reply.started":"2025-09-26T10:48:18.091649Z","shell.execute_reply":"2025-09-26T10:48:23.756744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install intel_extension_for_pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:48:23.758498Z","iopub.execute_input":"2025-09-26T10:48:23.758776Z","iopub.status.idle":"2025-09-26T10:48:29.398906Z","shell.execute_reply.started":"2025-09-26T10:48:23.758749Z","shell.execute_reply":"2025-09-26T10:48:29.398205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%writefile inference.py\n\n# import random\n# import multiprocessing as mp\n# import numpy as np\n# import pandas as pd\n# import torch\n# import vllm\n# import os\n\n# from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n# from utils import build_dataset\n# from constants import (\n#     BASE_MODEL_PATH)\n\n# def _filter_single_token_aliases(tokenizer, candidates):\n#     keep = []\n#     for s in candidates:\n#         ids = tokenizer.encode(s, add_special_tokens=False)\n#         if len(ids) == 1:\n#             keep.append(s)\n#     return keep\n\n\n# def _fallback_single_token(tokenizer, word: str) -> str:\n#     spaced = \" \" + word\n#     if len(tokenizer.encode(spaced, add_special_tokens=False)) == 1:\n#         return spaced\n#     if len(tokenizer.encode(word, add_special_tokens=False)) == 1:\n#         return word\n#     return spaced\n\n# def run_inference_on_device(df_slice: pd.DataFrame, device_id: int) -> pd.DataFrame:\n#     llm = vllm.LLM(\n#         BASE_MODEL_PATH,\n#         #quantization=\"awq\",\n#         tensor_parallel_size=1,\n#         gpu_memory_utilization=0.98,\n#         trust_remote_code=True,\n#         dtype=\"half\",\n#         enforce_eager=True,\n#         max_model_len=2836,\n#         disable_log_stats=True,\n#         enable_prefix_caching=True,\n#         max_lora_rank=64,\n#     )\n    \n#     tokenizer = llm.get_tokenizer()\n#     test_dataset = build_dataset(df_slice)\n#     texts = test_dataset[\"prompt\"]\n    \n#     yes_candidates = [\"Yes\", \" yes\", \"yes\", \"Yes.\", ...]\n#     no_candidates  = [\"No\",  \" no\",  \"no\", \"No.\", ...]\n\n#     yes_alias = _filter_single_token_aliases(tokenizer, yes_candidates)\n#     no_alias  = _filter_single_token_aliases(tokenizer, no_candidates)\n#     if not yes_alias:\n#         yes_alias = [_fallback_single_token(tokenizer, POSITIVE_ANSWER)]\n#     if not no_alias:\n#         no_alias = [_fallback_single_token(tokenizer, NEGATIVE_ANSWER)]\n\n#     choices = yes_alias + no_alias\n#     mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=choices)\n#     outputs = llm.generate(\n#         texts,\n#         vllm.SamplingParams(\n#             temperature=0.0,\n#             skip_special_tokens=True,\n#             max_tokens=1,\n#             logits_processors=[mclp],\n#             logprobs=len(choices),\n#         ),\n#         use_tqdm=True,\n#     )\n\n#     rows = []\n#     for out, rid in zip(outputs, df_slice[\"row_id\"].values):\n#         lp0 = {t.decoded_token: t.logprob for t in out.outputs[0].logprobs[0].values()}\n#         p_yes = sum(np.exp(lp0.get(tok, -1e9)) for tok in yes_alias)\n#         p_no  = sum(np.exp(lp0.get(tok, -1e9)) for tok in no_alias)\n#         score = p_yes / (p_yes + p_no + 1e-12)\n#         rows.append({\"row_id\": rid, \"rule_violation\": float(score)})\n#     predictions = pd.DataFrame(rows, columns=[\"row_id\", \"rule_violation\"])\n#     return predictions\n\n# def worker(device_id: int, df_slice: pd.DataFrame, return_dict):\n#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n#     print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n#     preds = run_inference_on_device(df_slice, device_id)\n#     return_dict[device_id] = preds\n    \n# def main():\n#     random.seed(42)\n#     np.random.seed(42)\n#     DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\n#     test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n#     test_dataframe[\"positive_example\"] = test_dataframe.apply(\n#         lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1\n#     )\n#     test_dataframe[\"negative_example\"] = test_dataframe.apply(\n#         lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1\n#     )\n\n#     test_dataframe = test_dataframe.drop(\n#         columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n#         errors=\"ignore\"\n#     ).reset_index(drop=True)\n#     n_gpus = max(1, torch.cuda.device_count())\n#     n_procs = min(2, n_gpus)\n\n#     df_slices = np.array_split(test_dataframe, n_procs)\n#     for i in range(n_procs):\n#         df_slices[i] = df_slices[i].reset_index(drop=True)\n\n#     if n_procs == 1:\n#         os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n#         predictions = run_inference_on_device(df_slices[0], 0)\n#     else:\n#         manager = mp.Manager()\n#         return_dict = manager.dict()\n#         procs = []\n#         for dev_id in range(n_procs):\n#             p = mp.Process(target=worker, args=(dev_id, df_slices[dev_id], return_dict))\n#             p.start()\n#             procs.append(p)\n#         for p in procs:\n#             p.join()\n#         predictions = pd.concat([return_dict[i] for i in range(n_procs)], ignore_index=True)\n#     submission = predictions[[\"row_id\", \"rule_violation\"]].copy()\n#     submission.to_csv(\"submission_qwen.csv\", index=False)\n#     print(\"✅ Saved submission_qwen.csv\")\n\n# if __name__ == \"__main__\":\n#     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:48:29.399954Z","iopub.execute_input":"2025-09-26T10:48:29.40019Z","iopub.status.idle":"2025-09-26T10:48:29.407539Z","shell.execute_reply.started":"2025-09-26T10:48:29.400165Z","shell.execute_reply":"2025-09-26T10:48:29.406847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile inference.py\nimport torch\nimport pandas as pd\nimport random\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\nfrom utils import build_dataset\nfrom constants import BASE_MODEL_PATH, LORA_PATH\n\ndef _filter_single_token_aliases(tokenizer, candidates):\n    \"\"\"Keep only aliases that tokenize to a single token.\"\"\"\n    keep = []\n    for s in candidates:\n        ids = tokenizer.encode(s, add_special_tokens=False)\n        if len(ids) == 1:\n            keep.append(s)\n    return keep\n\ndef _fallback_single_token(tokenizer, word: str) -> str:\n    spaced = \" \" + word\n    if len(tokenizer.encode(spaced, add_special_tokens=False)) == 1:\n        return spaced\n    if len(tokenizer.encode(word, add_special_tokens=False)) == 1:\n        return word\n    return spaced\n\ndef main():\n    DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\n    # Load model in 4-bit with LoRA adapter\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    base_model = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL_PATH,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    model = PeftModel.from_pretrained(base_model, LORA_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Load test data\n    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_df[\"positive_example\"] = test_df.apply(\n        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1\n    )\n    test_df[\"negative_example\"] = test_df.apply(\n        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1\n    )\n    test_df = test_df.drop(\n        columns=[\"positive_example_1\",\"positive_example_2\",\"negative_example_1\",\"negative_example_2\"],\n        errors=\"ignore\"\n    ).reset_index(drop=True)\n\n    # Build dataset\n    test_dataset = build_dataset(test_df,is_train = False)\n    prompts = test_dataset[\"prompt\"]\n\n    # Yes/No aliases\n    yes_candidates = [\"Yes\", \" yes\", \"yes\", \"Yes.\"]\n    no_candidates  = [\"No\",  \" no\",  \"no\", \"No.\"]\n    yes_alias = _filter_single_token_aliases(tokenizer, yes_candidates)\n    no_alias  = _filter_single_token_aliases(tokenizer, no_candidates)\n    if not yes_alias:\n        yes_alias = [_fallback_single_token(tokenizer, \"yes\")]\n    if not no_alias:\n        no_alias = [_fallback_single_token(tokenizer, \"no\")]\n\n    results = []\n    for rid, text in zip(test_df[\"row_id\"].values, prompts):\n        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits[:, -1, :]  # only last-token logits\n\n        # Convert logits → probabilities for yes/no tokens\n        logprobs = torch.log_softmax(logits, dim=-1)[0].cpu().numpy()\n        p_yes = sum(np.exp(logprobs[tokenizer.encode(tok, add_special_tokens=False)[0]]) for tok in yes_alias)\n        p_no  = sum(np.exp(logprobs[tokenizer.encode(tok, add_special_tokens=False)[0]]) for tok in no_alias)\n        score = p_yes / (p_yes + p_no + 1e-12)\n\n        results.append({\"row_id\": rid, \"rule_violation\": float(score)})\n\n    submission = pd.DataFrame(results)\n    submission.to_csv(\"submission_qwen.csv\", index=False)\n    print(submission.head())\n    print(\"✅ Saved submission_qwen.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:31:35.18697Z","iopub.execute_input":"2025-09-26T11:31:35.187667Z","iopub.status.idle":"2025-09-26T11:31:35.194857Z","shell.execute_reply.started":"2025-09-26T11:31:35.187635Z","shell.execute_reply":"2025-09-26T11:31:35.194168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:31:36.406425Z","iopub.execute_input":"2025-09-26T11:31:36.407023Z","iopub.status.idle":"2025-09-26T11:31:38.400915Z","shell.execute_reply.started":"2025-09-26T11:31:36.406998Z","shell.execute_reply":"2025-09-26T11:31:38.399986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T11:31:40.174449Z","iopub.execute_input":"2025-09-26T11:31:40.17508Z","iopub.status.idle":"2025-09-26T11:32:00.705667Z","shell.execute_reply.started":"2025-09-26T11:31:40.175046Z","shell.execute_reply":"2025-09-26T11:32:00.704938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":426330,"sourceType":"modelInstanceVersion","modelInstanceId":347541,"modelId":368803}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile constants.py\nBASE_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nLORA_PATH = \"output/\"\nCOMPLETE = \"Answer:\"\nprompt = f\"You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Answer 'yes' or 'no' only.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:27:07.840709Z","iopub.execute_input":"2025-09-25T23:27:07.840924Z","iopub.status.idle":"2025-09-25T23:27:07.850029Z","shell.execute_reply.started":"2025-09-25T23:27:07.840899Z","shell.execute_reply":"2025-09-25T23:27:07.849281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntrain_data = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\nprint(len(train_data))\ntest_sample=    pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\").sample(frac=0.5,random_state = 42 )\n\nprint(len(test_sample))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch import Tensor\nimport kagglehub\n\n\n# Pooling function (same as before)\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n    # Check whether padding is on the left\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\n# OOP wrapper around tokenizer + model + pooling\nclass QwenEmbedder(nn.Module):\n    def __init__(self, model_dir: str, max_length: int = 8192, device: str = None):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, padding_side=\"left\")\n        self.model = AutoModel.from_pretrained(model_dir)\n        self.max_length = max_length\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to(self.device)  # move model to device\n\n    def forward(self, texts: list[str]) -> Tensor:\n        # Tokenize batch\n        batch_dict = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        ).to(self.device)\n\n        # Forward pass\n        outputs = self.model(**batch_dict)\n\n        # Pool to sequence embedding\n        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n\n        # Normalize embeddings\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings\n\nif __name__ == \"__main__\":\n    model_dir = kagglehub.model_download(\"qwen-lm/qwen-3-embedding/transformers/0.6b\")\n    embedder = QwenEmbedder(model_dir)\n\n    # Queries and docs\n    queries = [\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\",\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"\n    ]\n    documents = [\n        \"The capital of China is Beijing.\",\n        \"Gravity is a force that attracts two bodies towards each other...\"\n    ]\n\n    # Get embeddings\n    query_emb = embedder(queries)\n    doc_emb = embedder(documents)\n\n    # Compute similarity\n    scores = query_emb @ doc_emb.T\n    print(scores.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\n\nimport pandas as pd \nfrom constants import prompt, COMPLETE\nimport numpy as np \nimport random\nimport re\nfrom datasets import Dataset\n\nrandom.seed(42)\nnp.random.seed(42)\n\ndef url_to_semantics(txt : str) -> str:\n    if not isinstance(txt,str):\n        return \"\"\n    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n    urls = re.findall(url_pattern, txt)\n    \n    if not urls:\n        return \"\" \n\n    all_semantics = []\n    seen_semantics = set()\n\n    for url in urls:\n        url_lower = url.lower()\n        \n        domain_match = re.search(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\", url_lower)\n        if domain_match:\n            full_domain = domain_match.group(1)\n            parts = full_domain.split('.')\n            for part in parts:\n                if part and part not in seen_semantics and len(part) > 3: # Avoid short parts like 'www'\n                    all_semantics.append(f\"domain:{part}\")\n                    seen_semantics.add(part)\n\n        # Extract path parts\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", url_lower)\n        path_parts = [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()] # Split by common delimiters\n\n        for part in path_parts:\n            # Clean up potential file extensions or query params\n            part_clean = re.sub(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\", \"\", part)\n            if part_clean and part_clean not in seen_semantics and len(part_clean) > 3:\n                all_semantics.append(f\"path:{part_clean}\")\n                seen_semantics.add(part_clean)\n\n    if not all_semantics:\n        return \"\"\n\n    return f\"\\nURL Keywords: {' '.join(all_semantics)}\"\n\ndef build_prompt(row):\n    subreddit = row.get(\"subreddit\", \"unknown\")\n    rule = row.get(\"rule\", \"\")\n    pos_example = row.get(\"positive_example\", \"\")\n    neg_example = row.get(\"negative_example\", \"\")\n    body = row.get(\"body\", \"\")\n    url_features_body = url_to_semantics(body)\n    url_features_pos = url_to_semantics(pos_example)\n    url_features_neg = url_to_semantics(neg_example)\n    return f\"\"\"\n{prompt}\n\nr/{subreddit} \nrule: {rule}\nExamples : \n1) {pos_example}{url_features_pos}\n{COMPLETE} yes\n2) {neg_example}{url_features_neg}\n{COMPLETE} no\n\n------\nComment: {body}{url_features_body}\n{COMPLETE} \"\"\"\n\n\ndef get_data_for_training(fpath,sample_frac = 0.5):\n    train_data = pd.read_csv(f\"{fpath}/train.csv\")\n    \n    test_df= pd.read_csv(f\"{fpath}/test.csv\").sample(frac=sample_frac,random_state = 42 )\n\n    \n\n\n    train_df = train_data[['body','rule','subreddit','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2','rule_violation']]\n    \n    #randomly assign examples\n    train_df['positive_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['positive_example_1'],train_df['positive_example_2'])\n    train_df['negative_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['negative_example_1'], train_df['negative_example_2'])\n    train_df.drop(columns = ['positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'], inplace = True)\n\n    dfs = [train_df]\n    \n    # build test df \n    \n    for rule_violation in ['yes', 'no']:\n        for i in range(1,3): #loop through both examples\n            subdf =  test_df.copy().drop(columns=['body','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'])\n\n            if rule_violation == 'yes':   # case when rule is violated \n                subdf['body'] = test_df[f'positive_example_{i}']\n                subdf['positive_example'] = test_df[f'positive_example_{3-i}']\n                subdf['negative_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'negative_example_{i}'],test_df[f'negative_example_{3-i}'])\n                subdf['rule_violation'] = 1\n            else:  # case when rule is not violated \n                subdf['body'] = test_df[f'negative_example_{i}']\n                subdf['positive_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'positive_example_{i}'],test_df[f'positive_example_{3-i}'])\n\n                subdf['neagtive_example'] = test_df[f'negative_example_{3-i}']\n                subdf['rule_violation'] = 0\n            dfs.append(subdf)\n\n    df =  pd.concat(dfs, axis = 0).drop_duplicates(ignore_index = True)\n    \n    return df\n\ndef build_dataset(df):\n    df['prompt'] = df.apply(build_prompt, axis = 1)\n\n    df['completion'] = df['rule_violation'].map(\n        {\n            1 : 'yes',\n            0 : 'no'\n        }\n    )\n    df = df[['prompt','completion']]\n\n    print(df)\n\n    dataset = Dataset.from_pandas(df)\n    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:27:19.979333Z","iopub.execute_input":"2025-09-25T23:27:19.979647Z","iopub.status.idle":"2025-09-25T23:27:19.987182Z","shell.execute_reply.started":"2025-09-25T23:27:19.979623Z","shell.execute_reply":"2025-09-25T23:27:19.986091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:27:36.540691Z","iopub.execute_input":"2025-09-25T23:27:36.541443Z","iopub.status.idle":"2025-09-25T23:29:13.833248Z","shell.execute_reply.started":"2025-09-25T23:27:36.541418Z","shell.execute_reply":"2025-09-25T23:29:13.832425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:29:15.37751Z","iopub.execute_input":"2025-09-25T23:29:15.377811Z","iopub.status.idle":"2025-09-25T23:29:46.889109Z","shell.execute_reply.started":"2025-09-25T23:29:15.377783Z","shell.execute_reply":"2025-09-25T23:29:46.888525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:29:54.669249Z","iopub.execute_input":"2025-09-25T23:29:54.6696Z","iopub.status.idle":"2025-09-25T23:30:00.964417Z","shell.execute_reply.started":"2025-09-25T23:29:54.669577Z","shell.execute_reply":"2025-09-25T23:30:00.963575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, HTML\nfrom utils import get_data_for_training, build_dataset, build_prompt, url_to_semantics\n\n# Lora imports\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom tqdm.auto import tqdm\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom constants import LORA_PATH, BASE_MODEL_PATH\n\n\ndef main():\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    df = get_data_for_training(data_path)\n    train_dataset = build_dataset(df)\n    df_train = pd.DataFrame(train_dataset)\n\n    df_train = pd.DataFrame(train_dataset)\n\n    lora_config = LoraConfig(\n        r = 16,\n        lora_alpha= 32,\n        lora_dropout = 0.1,\n        bias = \"none\",\n        target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type = \"CAUSAL_LM\")\n    \n    training_args = SFTConfig(\n        num_train_epochs = 1,\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 4,\n        optim = \"paged_adamw_8bit\",\n        learning_rate = 5e-5,\n        weight_decay = 0.01,\n        max_grad_norm = 1.0,\n\n        lr_scheduler_type = \"cosine\",\n        warmup_ratio=0.05,\n\n        bf16= is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n\n        save_strategy= \"no\",\n        report_to = \"none\",\n\n        completion_only_loss = True,\n        packing = True,\n        remove_unused_columns = False\n    )\n    \n    trainer = SFTTrainer(\n        BASE_MODEL_PATH,\n        args = training_args,\n        train_dataset = train_dataset,\n        peft_config = lora_config\n    )\n\n    trainer.train()\n    trainer.save_model(LORA_PATH)\n    #print(df_train.head(10))\n    \n\nif __name__ == \"__main__\":\n    main()\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:30:59.221975Z","iopub.execute_input":"2025-09-25T23:30:59.222827Z","iopub.status.idle":"2025-09-25T23:30:59.230037Z","shell.execute_reply.started":"2025-09-25T23:30:59.222791Z","shell.execute_reply":"2025-09-25T23:30:59.229067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:16:14.6518Z","iopub.execute_input":"2025-09-26T00:16:14.6521Z","iopub.status.idle":"2025-09-26T00:20:59.055396Z","shell.execute_reply.started":"2025-09-26T00:16:14.652074Z","shell.execute_reply":"2025-09-26T00:20:59.054535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install logits_processor_zoo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:21:45.34108Z","iopub.execute_input":"2025-09-26T00:21:45.341821Z","iopub.status.idle":"2025-09-26T00:21:49.60651Z","shell.execute_reply.started":"2025-09-26T00:21:45.341789Z","shell.execute_reply":"2025-09-26T00:21:49.605351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile inference.py\n\nimport random\nimport multiprocessing as mp\nimport numpy as np\nimport pandas as pd\nimport torch\nimport vllm\nimport os\n\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom utils import build_dataset\nfrom constants import (\n    BASE_MODEL_PATH)\n\ndef _filter_single_token_aliases(tokenizer, candidates):\n    keep = []\n    for s in candidates:\n        ids = tokenizer.encode(s, add_special_tokens=False)\n        if len(ids) == 1:\n            keep.append(s)\n    return keep\n\n\ndef _fallback_single_token(tokenizer, word: str) -> str:\n    spaced = \" \" + word\n    if len(tokenizer.encode(spaced, add_special_tokens=False)) == 1:\n        return spaced\n    if len(tokenizer.encode(word, add_special_tokens=False)) == 1:\n        return word\n    return spaced\n\ndef run_inference_on_device(df_slice: pd.DataFrame, device_id: int) -> pd.DataFrame:\n    llm = vllm.LLM(\n        BASE_MODEL_PATH,\n        #quantization=\"awq\",\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2836,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        max_lora_rank=64,\n    )\n    \n    tokenizer = llm.get_tokenizer()\n    test_dataset = build_dataset(df_slice)\n    texts = test_dataset[\"prompt\"]\n    \n    yes_candidates = [\"Yes\", \" yes\", \"yes\", \"Yes.\", ...]\n    no_candidates  = [\"No\",  \" no\",  \"no\", \"No.\", ...]\n\n    yes_alias = _filter_single_token_aliases(tokenizer, yes_candidates)\n    no_alias  = _filter_single_token_aliases(tokenizer, no_candidates)\n    if not yes_alias:\n        yes_alias = [_fallback_single_token(tokenizer, POSITIVE_ANSWER)]\n    if not no_alias:\n        no_alias = [_fallback_single_token(tokenizer, NEGATIVE_ANSWER)]\n\n    choices = yes_alias + no_alias\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=choices)\n    outputs = llm.generate(\n        texts,\n        vllm.SamplingParams(\n            temperature=0.0,\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=len(choices),\n        ),\n        use_tqdm=True,\n    )\n\n    rows = []\n    for out, rid in zip(outputs, df_slice[\"row_id\"].values):\n        lp0 = {t.decoded_token: t.logprob for t in out.outputs[0].logprobs[0].values()}\n        p_yes = sum(np.exp(lp0.get(tok, -1e9)) for tok in yes_alias)\n        p_no  = sum(np.exp(lp0.get(tok, -1e9)) for tok in no_alias)\n        score = p_yes / (p_yes + p_no + 1e-12)\n        rows.append({\"row_id\": rid, \"rule_violation\": float(score)})\n    predictions = pd.DataFrame(rows, columns=[\"row_id\", \"rule_violation\"])\n    return predictions\n\ndef worker(device_id: int, df_slice: pd.DataFrame, return_dict):\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n    preds = run_inference_on_device(df_slice, device_id)\n    return_dict[device_id] = preds\n    \ndef main():\n    random.seed(42)\n    np.random.seed(42)\n    DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1\n    )\n    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1\n    )\n\n    test_dataframe = test_dataframe.drop(\n        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n        errors=\"ignore\"\n    ).reset_index(drop=True)\n    n_gpus = max(1, torch.cuda.device_count())\n    n_procs = min(2, n_gpus)\n\n    df_slices = np.array_split(test_dataframe, n_procs)\n    for i in range(n_procs):\n        df_slices[i] = df_slices[i].reset_index(drop=True)\n\n    if n_procs == 1:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        predictions = run_inference_on_device(df_slices[0], 0)\n    else:\n        manager = mp.Manager()\n        return_dict = manager.dict()\n        procs = []\n        for dev_id in range(n_procs):\n            p = mp.Process(target=worker, args=(dev_id, df_slices[dev_id], return_dict))\n            p.start()\n            procs.append(p)\n        for p in procs:\n            p.join()\n        predictions = pd.concat([return_dict[i] for i in range(n_procs)], ignore_index=True)\n    submission = predictions[[\"row_id\", \"rule_violation\"]].copy()\n    submission.to_csv(\"submission_qwen.csv\", index=False)\n    print(\"✅ Saved submission_qwen.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:26:00.330354Z","iopub.execute_input":"2025-09-26T00:26:00.330705Z","iopub.status.idle":"2025-09-26T00:26:00.339299Z","shell.execute_reply.started":"2025-09-26T00:26:00.330673Z","shell.execute_reply":"2025-09-26T00:26:00.338311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:31:59.334686Z","iopub.execute_input":"2025-09-25T23:31:59.334986Z","iopub.status.idle":"2025-09-25T23:53:41.077362Z","shell.execute_reply.started":"2025-09-25T23:31:59.334966Z","shell.execute_reply":"2025-09-25T23:53:41.076151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T00:26:06.00899Z","iopub.execute_input":"2025-09-26T00:26:06.009754Z","iopub.status.idle":"2025-09-26T00:27:17.079353Z","shell.execute_reply.started":"2025-09-26T00:26:06.009726Z","shell.execute_reply":"2025-09-26T00:27:17.0782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
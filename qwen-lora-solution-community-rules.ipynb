{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":363127,"sourceType":"modelInstanceVersion","modelInstanceId":301507,"modelId":322000},{"sourceId":391622,"sourceType":"modelInstanceVersion","modelInstanceId":322458,"modelId":322000},{"sourceId":426330,"sourceType":"modelInstanceVersion","modelInstanceId":347541,"modelId":368803}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation\n!pip install trl\n!pip install bitsandbytes\n!pip install vllm\n!pip install logits_processor_zoo\n!pip install autoawq\n\n!pip install intel_extension_for_pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:22:28.219653Z","iopub.execute_input":"2025-09-30T21:22:28.219874Z","iopub.status.idle":"2025-09-30T21:29:44.384129Z","shell.execute_reply.started":"2025-09-30T21:22:28.219825Z","shell.execute_reply":"2025-09-30T21:29:44.383219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:29:44.388412Z","iopub.execute_input":"2025-09-30T21:29:44.388857Z","iopub.status.idle":"2025-09-30T21:30:10.474335Z","shell.execute_reply.started":"2025-09-30T21:29:44.388801Z","shell.execute_reply":"2025-09-30T21:30:10.473742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile constants.py\n#BASE_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nBASE_MODEL_PATH = \"/kaggle/input/qwen-3/transformers/0.6b-base/1\"\n\nLORA_PATH = \"output/\"\nCOMPLETE = \"Answer:\"\npositive = 'yes'\nnegative = 'no'\nprompt = f\"You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule.\"\noutput_fmt = {\n    \"Answer: Yes / No\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:30:29.701445Z","iopub.execute_input":"2025-09-30T21:30:29.7017Z","iopub.status.idle":"2025-09-30T21:30:29.706443Z","shell.execute_reply.started":"2025-09-30T21:30:29.701678Z","shell.execute_reply":"2025-09-30T21:30:29.705822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntrain_data = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\n\nprint(len(train_data))\ntest_sample=    pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\").sample(frac=0.5,random_state = 42 )\n\nprint(len(test_sample))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:30:43.007473Z","iopub.execute_input":"2025-09-30T21:30:43.008035Z","iopub.status.idle":"2025-09-30T21:30:43.10898Z","shell.execute_reply.started":"2025-09-30T21:30:43.008005Z","shell.execute_reply":"2025-09-30T21:30:43.108034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_data['rule_violation'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T23:08:04.6837Z","iopub.execute_input":"2025-09-30T23:08:04.684451Z","iopub.status.idle":"2025-09-30T23:08:04.69454Z","shell.execute_reply.started":"2025-09-30T23:08:04.684423Z","shell.execute_reply":"2025-09-30T23:08:04.69375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:31:09.749508Z","iopub.execute_input":"2025-09-30T21:31:09.750147Z","iopub.status.idle":"2025-09-30T21:31:09.760099Z","shell.execute_reply.started":"2025-09-30T21:31:09.750116Z","shell.execute_reply":"2025-09-30T21:31:09.759314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:31:29.302404Z","iopub.execute_input":"2025-09-30T21:31:29.303099Z","iopub.status.idle":"2025-09-30T21:31:32.427113Z","shell.execute_reply.started":"2025-09-30T21:31:29.303071Z","shell.execute_reply":"2025-09-30T21:31:32.426388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch import Tensor\nimport kagglehub\n\n\n# Pooling function (same as before)\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n    # Check whether padding is on the left\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\n# OOP wrapper around tokenizer + model + pooling\nclass QwenEmbedder(nn.Module):\n    def __init__(self, model_dir: str, max_length: int = 8192, device: str = None):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, padding_side=\"left\")\n        self.model = AutoModel.from_pretrained(model_dir)\n        self.max_length = max_length\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to(self.device)  # move model to device\n\n    def forward(self, texts: list[str]) -> Tensor:\n        # Tokenize batch\n        batch_dict = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        ).to(self.device)\n\n        # Forward pass\n        outputs = self.model(**batch_dict)\n\n        # Pool to sequence embedding\n        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n\n        # Normalize embeddings\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings\n\nif __name__ == \"__main__\":\n    model_dir = kagglehub.model_download(\"qwen-lm/qwen-3-embedding/transformers/0.6b\")\n    embedder = QwenEmbedder(model_dir)\n\n    # Queries and docs\n    queries = [\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\",\n        \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"\n    ]\n    documents = [\n        \"The capital of China is Beijing.\",\n        \"Gravity is a force that attracts two bodies towards each other...\"\n    ]\n\n    # Get embeddings\n    query_emb = embedder(queries)\n    doc_emb = embedder(documents)\n\n    # Compute similarity\n    scores = query_emb @ doc_emb.T\n    print(scores.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\n\nimport pandas as pd \nfrom constants import prompt, COMPLETE\nimport numpy as np \nimport random\nimport re\nfrom datasets import Dataset\n\nrandom.seed(42)\nnp.random.seed(42)\n\ndef url_to_semantics(txt : str) -> str:\n    if not isinstance(txt,str):\n        return \"\"\n    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n    urls = re.findall(url_pattern, txt)\n    \n    if not urls:\n        return \"\" \n\n    all_semantics = []\n    seen_semantics = set()\n\n    for url in urls:\n        url_lower = url.lower()\n        \n        domain_match = re.search(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\", url_lower)\n        if domain_match:\n            full_domain = domain_match.group(1)\n            parts = full_domain.split('.')\n            for part in parts:\n                if part and part not in seen_semantics and len(part) > 3: # Avoid short parts like 'www'\n                    all_semantics.append(f\"domain:{part}\")\n                    seen_semantics.add(part)\n\n        # Extract path parts\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", url_lower)\n        path_parts = [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()] # Split by common delimiters\n\n        for part in path_parts:\n            # Clean up potential file extensions or query params\n            part_clean = re.sub(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\", \"\", part)\n            if part_clean and part_clean not in seen_semantics and len(part_clean) > 3:\n                all_semantics.append(f\"path:{part_clean}\")\n                seen_semantics.add(part_clean)\n\n    if not all_semantics:\n        return \"\"\n\n    return f\"\\nURL Keywords: {' '.join(all_semantics)}\"\n\ndef build_prompt(row):\n    subreddit = row.get(\"subreddit\", \"unknown\")\n    rule = row.get(\"rule\", \"\")\n    pos_example = row.get(\"positive_example\", \"\")\n    neg_example = row.get(\"negative_example\", \"\")\n    body = row.get(\"body\", \"\")\n    url_features_body = url_to_semantics(body)\n    url_features_pos = url_to_semantics(pos_example)\n    url_features_neg = url_to_semantics(neg_example)\n    return f\"\"\"\n{prompt}\n\nr/{subreddit} \nrule: {rule}\nExamples : \n1) {pos_example}{url_features_pos}\n{COMPLETE} yes\n2) {neg_example}{url_features_neg}\n{COMPLETE} no\n\n------\nComment: {body}{url_features_body}\n{COMPLETE} \"\"\"\n\n\ndef get_data_for_training(fpath,sample_frac = 0.5):\n    train_data = pd.read_csv(f\"{fpath}/train.csv\")\n    \n    test_df= pd.read_csv(f\"{fpath}/test.csv\").sample(frac=sample_frac,random_state = 42 )\n\n    \n\n\n    train_df = train_data[['body','rule','subreddit','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2','rule_violation']]\n    \n    #randomly assign examples\n    train_df['positive_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['positive_example_1'],train_df['positive_example_2'])\n    train_df['negative_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['negative_example_1'], train_df['negative_example_2'])\n    train_df.drop(columns = ['positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'], inplace = True)\n\n    dfs = [train_df]\n    \n    # build test df \n    \n    for rule_violation in ['yes', 'no']:\n        for i in range(1,3): #loop through both examples\n            subdf =  test_df.copy().drop(columns=['body','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'])\n\n            if rule_v\n    test_df= pd.read_csv(f\"{fpath}/test.csv\").sample(frac=sample_frac,random_state = 42 )\n\n    \n\n\n    train_df = train_data[['body','rule','subreddit','positive_example_1','positive_example_2', 'negative_example_1','negative_example_2','rule_violation']]\n    \n    #randomly assign examples\n    train_df['positive_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['positive_example_1'],train_df['positive_example_2'])\n    train_df['negative_example'] = np.where(np.random.rand(len(train_df)) <0.5 , train_df['negative_example_1'], train_df['negative_example_2'])\n    train_df.drop(columns = ['positive_example_1','positive_example_2', 'negative_example_1','negative_example_2'], inplace = True)\n\n    dfs = [train_df]\n    \n    # build test df \n    \n    for rule_violation in ['yes', 'no']:\n        for i in range(1,3): #loop through both examples\n            subdf = iolation == 'yes':   # case when rule is violated \n                subdf['body'] = test_df[f'positive_example_{i}']\n                subdf['positive_example'] = test_df[f'positive_example_{3-i}']\n                subdf['negative_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'negative_example_{i}'],test_df[f'negative_example_{3-i}'])\n                subdf['rule_violation'] = 1\n            else:  # case when rule is not violated \n                subdf['body'] = test_df[f'negative_example_{i}']\n                subdf['positive_example'] = np.where(np.random.rand(len(test_df))<0.5, test_df[f'positive_example_{i}'],test_df[f'positive_example_{3-i}'])\n\n                subdf['neagtive_example'] = test_df[f'negative_example_{3-i}']\n                subdf['rule_violation'] = 0\n            dfs.append(subdf)\n\n    df =  pd.concat(dfs, axis = 0).drop_duplicates(ignore_index = True)\n    \n    return df\n\ndef build_dataset(df,is_train = True):\n    df['prompt'] = df.apply(build_prompt, axis = 1)\n\n    if is_train:\n\n        df['completion'] = df['rule_violation'].map(\n            {\n                1 : 'yes',\n                0 : 'no'\n            }\n        )\n        df = df[['prompt','completion']]\n    else: \n        df = df[['prompt']]\n\n    print(df)\n\n    dataset = Dataset.from_pandas(df)\n    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:31:49.496555Z","iopub.execute_input":"2025-09-30T21:31:49.496861Z","iopub.status.idle":"2025-09-30T21:31:49.504421Z","shell.execute_reply.started":"2025-09-30T21:31:49.496811Z","shell.execute_reply":"2025-09-30T21:31:49.503685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T23:33:39.720591Z","iopub.execute_input":"2025-09-29T23:33:39.72132Z","iopub.status.idle":"2025-09-29T23:33:39.725723Z","shell.execute_reply.started":"2025-09-29T23:33:39.721285Z","shell.execute_reply":"2025-09-29T23:33:39.724963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T23:33:39.726604Z","iopub.execute_input":"2025-09-29T23:33:39.726851Z","iopub.status.idle":"2025-09-29T23:33:43.064306Z","shell.execute_reply.started":"2025-09-29T23:33:39.726822Z","shell.execute_reply":"2025-09-29T23:33:43.063374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, HTML\nfrom utils import get_data_for_training, build_dataset, build_prompt, url_to_semantics\n\n# Lora imports\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom tqdm.auto import tqdm\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom constants import LORA_PATH, BASE_MODEL_PATH\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import ShuffleSplit\n\n\nfrom sklern.metrics import accuracy_score,log_loss\n\n\ndef evaluate_model(model, tokenizer, val_df):\n    val_data = build_dataset(val_df, is_train=True)\n    prompts = val_data['prompt']\n    targets = val_data['completion']\n    \n    preds = []\n    for prompt in tqdm(prompts, desc=\"Evaluating\"):\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n        with torch.no_grad():\n            output = model.generate(**inputs, max_new_tokens=1)\n        decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n        if 'yes' in decoded:\n            preds.append('yes')\n        elif 'no' in decoded:\n            preds.append('no')\n        else:\n            preds.append('')  # fallback for misgenerated output\n\n    # Map to binary for metrics\n    y_true = [1 if label == 'yes' else 0 for label in targets]\n    y_pred = [1 if pred == 'yes' else 0 for pred in preds]\n\n    # For log loss, estimate pseudo-probabilities\n    probs = [0.9 if pred == 'yes' else 0.1 for pred in preds]\n\n    acc = accuracy_score(y_true, y_pred)\n    bce = log_loss(y_true, probs, eps=1e-7)\n\n    return acc, bce\n\ndef train_fold(train_df, val_df, fold_num):\n    # Convert to HF Dataset\n    train_data = build_dataset(train_df, is_train=True)\n\n    # LoRA config\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.1,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\"\n    )\n\n    # Training config\n    training_args = SFTConfig(\n        num_train_epochs=1,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_8bit\",\n        learning_rate=5e-5,\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.05,\n        bf16=is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n        save_strategy=\"no\",\n        report_to=\"none\",\n        completion_only_loss=True,\n        packing=True,\n        remove_unused_columns=False\n    )\n\n    # Initialize trainer\n    trainer = SFTTrainer(\n        BASE_MODEL_PATH,\n        args=training_args,\n        train_dataset=train_data,\n        peft_config=lora_config\n    )\n\n    print(f\"\\n========== Training Fold {fold_num} ==========\")\n    trainer.train()\n\n    # Save LoRA model\n    fold_lora_path = os.path.join(LORA_PATH, f\"fold_{fold_num}\")\n    trainer.save_model(fold_lora_path)\n\n    # Convert to fp16\n    base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, device_map=\"auto\")\n    lora_model = PeftModel.from_pretrained(base_model, fold_lora_path)\n    lora_model.save_pretrained(f\"{fold_lora_path}_fp16\", safe_serialization=True)\n\n    # Evaluation\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n    acc, bce = evaluate_model(lora_model, tokenizer, val_df)\n\n    print(f\"\\n✅ Fold {fold_num} Evaluation:\")\n    print(f\"   Accuracy: {acc:.4f}\")\n    print(f\"   BCE Loss: {bce:.4f}\")\n\n    return acc, bce\n\n\n\ndef main():\n    import torch  # Needed for device transfer\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    df = get_data_for_training(data_path)\n\n    # K-Fold Cross-Validation\n    k_folds = 5\n    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n\n    all_acc = []\n    all_bce = []\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n        train_df = df.iloc[train_idx].copy()\n        val_df = df.iloc[val_idx].copy()\n\n        acc, bce = train_fold(train_df, val_df, fold + 1)\n        all_acc.append(acc)\n        all_bce.append(bce)\n\n    print(\"\\n========== Final Cross-Validation Results ==========\")\n    print(f\"Average Accuracy: {np.mean(all_acc):.4f} ± {np.std(all_acc):.4f}\")\n    print(f\"Average BCE Loss: {np.mean(all_bce):.4f} ± {np.std(all_bce):.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T23:12:25.830911Z","iopub.execute_input":"2025-09-30T23:12:25.83163Z","iopub.status.idle":"2025-09-30T23:12:25.838257Z","shell.execute_reply.started":"2025-09-30T23:12:25.831604Z","shell.execute_reply":"2025-09-30T23:12:25.837436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:43:40.897022Z","iopub.execute_input":"2025-09-26T10:43:40.897313Z","iopub.status.idle":"2025-09-26T10:48:09.655805Z","shell.execute_reply.started":"2025-09-26T10:43:40.897288Z","shell.execute_reply":"2025-09-26T10:48:09.654787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install logits_processor_zoo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:35:17.879574Z","iopub.execute_input":"2025-09-30T21:35:17.879881Z","iopub.status.idle":"2025-09-30T21:35:21.18095Z","shell.execute_reply.started":"2025-09-30T21:35:17.879803Z","shell.execute_reply":"2025-09-30T21:35:21.180046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install autoawq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:48:18.091421Z","iopub.execute_input":"2025-09-26T10:48:18.091676Z","iopub.status.idle":"2025-09-26T10:48:23.757501Z","shell.execute_reply.started":"2025-09-26T10:48:18.091649Z","shell.execute_reply":"2025-09-26T10:48:23.756744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install intel_extension_for_pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:35:45.894096Z","iopub.execute_input":"2025-09-30T21:35:45.894377Z","iopub.status.idle":"2025-09-30T21:35:49.005231Z","shell.execute_reply.started":"2025-09-30T21:35:45.894353Z","shell.execute_reply":"2025-09-30T21:35:49.004301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%writefile inference.py\n\n# import random\n# import multiprocessing as mp\n# import numpy as np\n# import pandas as pd\n# import torch\n# import vllm\n# import os\n\n# from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n# from utils import build_dataset\n# from constants import (\n#     BASE_MODEL_PATH)\n\n# def _filter_single_token_aliases(tokenizer, candidates):\n#     keep = []\n#     for s in candidates:\n#         ids = tokenizer.encode(s, add_special_tokens=False)\n#         if len(ids) == 1:\n#             keep.append(s)\n#     return keep\n\n\n# def _fallback_single_token(tokenizer, word: str) -> str:\n#     spaced = \" \" + word\n#     if len(tokenizer.encode(spaced, add_special_tokens=False)) == 1:\n#         return spaced\n#     if len(tokenizer.encode(word, add_special_tokens=False)) == 1:\n#         return word\n#     return spaced\n\n# def run_inference_on_device(df_slice: pd.DataFrame, device_id: int) -> pd.DataFrame:\n#     llm = vllm.LLM(\n#         BASE_MODEL_PATH,\n#         #quantization=\"awq\",\n#         tensor_parallel_size=1,\n#         gpu_memory_utilization=0.98,\n#         trust_remote_code=True,\n#         dtype=\"half\",\n#         enforce_eager=True,\n#         max_model_len=2836,\n#         disable_log_stats=True,\n#         enable_prefix_caching=True,\n#         max_lora_rank=64,\n#     )\n    \n#     tokenizer = llm.get_tokenizer()\n#     test_dataset = build_dataset(df_slice)\n#     texts = test_dataset[\"prompt\"]\n    \n#     yes_candidates = [\"Yes\", \" yes\", \"yes\", \"Yes.\", ...]\n#     no_candidates  = [\"No\",  \" no\",  \"no\", \"No.\", ...]\n\n#     yes_alias = _filter_single_token_aliases(tokenizer, yes_candidates)\n#     no_alias  = _filter_single_token_aliases(tokenizer, no_candidates)\n#     if not yes_alias:\n#         yes_alias = [_fallback_single_token(tokenizer, POSITIVE_ANSWER)]\n#     if not no_alias:\n#         no_alias = [_fallback_single_token(tokenizer, NEGATIVE_ANSWER)]\n\n#     choices = yes_alias + no_alias\n#     mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=choices)\n#     outputs = llm.generate(\n#         texts,\n#         vllm.SamplingParams(\n#             temperature=0.0,\n#             skip_special_tokens=True,\n#             max_tokens=1,\n#             logits_processors=[mclp],\n#             logprobs=len(choices),\n#         ),\n#         use_tqdm=True,\n#     )\n\n#     rows = []\n#     for out, rid in zip(outputs, df_slice[\"row_id\"].values):\n#         lp0 = {t.decoded_token: t.logprob for t in out.outputs[0].logprobs[0].values()}\n#         p_yes = sum(np.exp(lp0.get(tok, -1e9)) for tok in yes_alias)\n#         p_no  = sum(np.exp(lp0.get(tok, -1e9)) for tok in no_alias)\n#         score = p_yes / (p_yes + p_no + 1e-12)\n#         rows.append({\"row_id\": rid, \"rule_violation\": float(score)})\n#     predictions = pd.DataFrame(rows, columns=[\"row_id\", \"rule_violation\"])\n#     return predictions\n\n# def worker(device_id: int, df_slice: pd.DataFrame, return_dict):\n#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n#     print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n#     preds = run_inference_on_device(df_slice, device_id)\n#     return_dict[device_id] = preds\n    \n# def main():\n#     random.seed(42)\n#     np.random.seed(42)\n#     DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\n#     test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n#     test_dataframe[\"positive_example\"] = test_dataframe.apply(\n#         lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1\n#     )\n#     test_dataframe[\"negative_example\"] = test_dataframe.apply(\n#         lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1\n#     )\n\n#     test_dataframe = test_dataframe.drop(\n#         columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n#         errors=\"ignore\"\n#     ).reset_index(drop=True)\n#     n_gpus = max(1, torch.cuda.device_count())\n#     n_procs = min(2, n_gpus)\n\n#     df_slices = np.array_split(test_dataframe, n_procs)\n#     for i in range(n_procs):\n#         df_slices[i] = df_slices[i].reset_index(drop=True)\n\n#     if n_procs == 1:\n#         os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n#         predictions = run_inference_on_device(df_slices[0], 0)\n#     else:\n#         manager = mp.Manager()\n#         return_dict = manager.dict()\n#         procs = []\n#         for dev_id in range(n_procs):\n#             p = mp.Process(target=worker, args=(dev_id, df_slices[dev_id], return_dict))\n#             p.start()\n#             procs.append(p)\n#         for p in procs:\n#             p.join()\n#         predictions = pd.concat([return_dict[i] for i in range(n_procs)], ignore_index=True)\n#     submission = predictions[[\"row_id\", \"rule_violation\"]].copy()\n#     submission.to_csv(\"submission_qwen.csv\", index=False)\n#     print(\"✅ Saved submission_qwen.csv\")\n\n# if __name__ == \"__main__\":\n#     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:48:29.399954Z","iopub.execute_input":"2025-09-26T10:48:29.40019Z","iopub.status.idle":"2025-09-26T10:48:29.407539Z","shell.execute_reply.started":"2025-09-26T10:48:29.400165Z","shell.execute_reply":"2025-09-26T10:48:29.406847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile inference.py\nimport torch\nimport pandas as pd\nimport random\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\nfrom utils import build_dataset\nfrom constants import BASE_MODEL_PATH, LORA_PATH\n\ndef _filter_single_token_aliases(tokenizer, candidates):\n    \"\"\"Keep only aliases that tokenize to a single token.\"\"\"\n    keep = []\n    for s in candidates:\n        ids = tokenizer.encode(s, add_special_tokens=False)\n        if len(ids) == 1:\n            keep.append(s)\n    return keep\n\ndef _fallback_single_token(tokenizer, word: str) -> str:\n    spaced = \" \" + word\n    if len(tokenizer.encode(spaced, add_special_tokens=False)) == 1:\n        return spaced\n    if len(tokenizer.encode(word, add_special_tokens=False)) == 1:\n        return word\n    return spaced\n\ndef main():\n    DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\n    # Load model in 4-bit with LoRA adapter\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    base_model = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL_PATH,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    model = PeftModel.from_pretrained(base_model, LORA_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Load test data\n    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_df[\"positive_example\"] = test_df.apply(\n        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1\n    )\n    test_df[\"negative_example\"] = test_df.apply(\n        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1\n    )\n    test_df = test_df.drop(\n        columns=[\"positive_example_1\",\"positive_example_2\",\"negative_example_1\",\"negative_example_2\"],\n        errors=\"ignore\"\n    ).reset_index(drop=True)\n\n    # Build dataset\n    test_dataset = build_dataset(test_df,is_train = False)\n    prompts = test_dataset[\"prompt\"]\n\n    )\n    model = PeftModel.from_pretrained(base_model, LORA_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Load test data\n    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_df[\"positive_example\"] = test_df.apply(\n        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1\n    )\n    test_df[\"negative_example\"] = test_df.apply(\n        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1\n    )\n    test_df = test_df.drop(\n        columns=[\"positive_example_1\",\"positive_example_2\",\"negative_example_1\",\"negative_example_2\"],\n        errors=\"ignore\"\n    # Yes/No aliases\n    yes_candidates = [\"Yes\", \" yes\", \"yes\", \"Yes.\"]\n    no_candidates  = [\"No\",  \" no\",  \"no\", \"No.\"]\n    yes_alias = _filter_single_token_aliases(tokenizer, yes_candidates)\n    no_alias  = _filter_single_token_aliases(tokenizer, no_candidates)\n    if not yes_alias:\n        yes_alias = [_fallback_single_token(tokenizer, \"yes\")]\n    if not no_alias:\n        no_alias = [_fallback_single_token(tokenizer, \"no\")]\n\n    results = []\n    for rid, text in zip(test_df[\"row_id\"].values, prompts):\n        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits[:, -1, :]  # only last-token logits\n\n        # Convert logits → probabilities for yes/no tokens\n        logprobs = torch.log_softmax(logits, dim=-1)[0].cpu().numpy()\n        p_yes = sum(np.exp(logprobs[tokenizer.encode(tok, add_special_tokens=False)[0]]) for tok in yes_alias)\n        p_no  = sum(np.exp(logprobs[tokenizer.encode(tok, add_special_tokens=False)[0]]) for tok in no_alias)\n        score = p_yes / (p_yes + p_no + 1e-12)\n\n        results.append({\"row_id\": rid, \"rule_violation\": float(score)})\n\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:36:16.12603Z","iopub.execute_input":"2025-09-30T21:36:16.126329Z","iopub.status.idle":"2025-09-30T21:36:16.133348Z","shell.execute_reply.started":"2025-09-30T21:36:16.126303Z","shell.execute_reply":"2025-09-30T21:36:16.132678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T23:12:55.8529Z","iopub.execute_input":"2025-09-30T23:12:55.853175Z","iopub.status.idle":"2025-09-30T23:13:08.941367Z","shell.execute_reply.started":"2025-09-30T23:12:55.853154Z","shell.execute_reply":"2025-09-30T23:13:08.940626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T00:07:57.063282Z","iopub.execute_input":"2025-09-30T00:07:57.063601Z","iopub.status.idle":"2025-09-30T00:08:17.577015Z","shell.execute_reply.started":"2025-09-30T00:07:57.063559Z","shell.execute_reply":"2025-09-30T00:08:17.576039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}